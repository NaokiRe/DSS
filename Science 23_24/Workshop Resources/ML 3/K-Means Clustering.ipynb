{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "630a5f73",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd769e",
   "metadata": {},
   "source": [
    "#### 1. Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aca922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938ce69",
   "metadata": {},
   "source": [
    "#### 2. Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf21037f",
   "metadata": {},
   "source": [
    "This dataset contains the physical characteristics of rocks to facilitate the predictions of lithothology from well logging measurments and was used for a Machine Learning competition hosted by FORCE 2020 and XEEK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924ef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "data = pd.read_csv('UL_Features.csv')\n",
    "\n",
    "# dropping some columns for our convinience\n",
    "df = data.drop(['Unnamed: 0', 'wellName', 'MD', 'CALI', 'RACEHM_l10', 'RPCEHM_l10', 'RACELM_l10', 'RPCELM_l10', 'VSH'], axis = 1)\n",
    "\n",
    "# dropping null values (if any)\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9963ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Lets see how our dataset looks like now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bdf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # This gives a quantitative description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace2103",
   "metadata": {},
   "source": [
    "#### 3. Transforming the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c760637",
   "metadata": {},
   "source": [
    "Standardise the data by using the StandardScalar function from sklearn.\n",
    "\n",
    "To account for variations in measurements and units, it is a common practise to standardise the data before applying the Machine learning model. \n",
    "\n",
    "The function used to do this is ad follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0324a9",
   "metadata": {},
   "source": [
    "$$z = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$x_i$ is each individual value (i.e., $72.7456$ in $1^{st}$ row of 1st column viewed by df)\n",
    "\n",
    "$\\mu$ is the mean of that column (i.e., $39.781249$ in the $2^{nd}$ row of the $1^{st}$ column which can be viewed by df.describe())\n",
    "\n",
    "$\\sigma$ is the standard deviation (i.e., $18.378084$ in the $3^{rd}$ row of the $1^{st}$ column which can be viewed by df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645999fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # This function automatically standardises each data point according to the formula given above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add new columns giving standardised values of each data point\n",
    "\n",
    "df[['GR_T', 'RHOB_T', 'NPHI_T', 'PHIF_T', 'SW_T']] = scaler.fit_transform(df[['GR', 'RHOB', 'NPHI', 'PHIF', 'SW']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69da359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Viewing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc5531",
   "metadata": {},
   "source": [
    "### 4. Clustering the Data\n",
    "#### Identifying the Optimum Number of Clusters: The Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a586e46",
   "metadata": {},
   "source": [
    "There are many different ways to find the optimum number of clusters to divide the data point into. The method we are going to use is called the Elbow Method. \n",
    "\n",
    "It plots the inertia which is a measure of how well the data is clustered by the K-Means algorithm, against the number of clusters. We are looking for a point where the inertia begins to slow down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df000ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to find the optimum number of clusters\n",
    "\n",
    "def optimise_k_means(data, max_k):\n",
    "    \n",
    "    k_values = []\n",
    "    inertias = []\n",
    "    \n",
    "    for k in range(1, max_k):\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(data)\n",
    "        \n",
    "        k_values.append(k)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "    # Generating Elbow plot\n",
    "    \n",
    "    fig = plt.subplots (figsize = (10, 5)) # Setting a suitable size of graph. Can be experimented with.\n",
    "    plt.plot(k_values, inertias, 'o-') # This is the main line of code to plot the graph, rest are just decorations\n",
    "    plt.xlabel('Number of Clusters') # Labelling the x axis\n",
    "    plt.ylabel('Inertia') # Labelling the y axis\n",
    "    plt.grid(True) # Introducing the grid\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf64046",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimise_k_means(df[['RHOB_T', 'NPHI_T']], 10) # Note that we are using the data transformed by SatandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimise_k_means(df[['PHIF_T', 'GR_T']], 10) K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c968a",
   "metadata": {},
   "source": [
    "#### 5. Applying K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3) # Setting k=3\n",
    "kmeans.fit(df[['RHOB_T', 'NPHI_T']]) # Fitting the data of two columns into the algorithm\n",
    "df['kmeans_3'] = kmeans.labels_ # Creating a new column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7755309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea0fdb",
   "metadata": {},
   "source": [
    "#### 6. Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x = df['NPHI'], y = df['RHOB'], c = df['kmeans_3'])\n",
    "plt.xlim(-0.1, 0.5)\n",
    "plt.ylim(3, 2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974abe1",
   "metadata": {},
   "source": [
    "#### 7. Creating Multiple Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 6):\n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    kmeans.fit(df[['NPHI_T', 'RHOB_T']])\n",
    "    df[f'KMeans_{k}'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ad525",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots (nrows = 1, ncols = 5, figsize = (20, 5))\n",
    "\n",
    "for i, ax in enumerate(fig.axes, start = 1):\n",
    "    ax.scatter(x = df['NPHI'], y = df['RHOB'], c = df[f'KMeans_{i}'])\n",
    "    ax.set_ylim(3, 2)\n",
    "    ax.set_xlim(-0.1, 0.5)\n",
    "    ax.set_title(f'N Cluster: {i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a series of $n$ datapoints, lets say $(x_1, y_1)$, $(x_2, y_2)$,...,$(x_n, y_n)$.  How do we find a linear (or straight) line that best fits the data?  In this workshop, we will go over how we will be able to find the line that best fits the data using the concept of Linear Regression and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Generate Some Random Data\n",
    "\n",
    "Of course, when we need to find the line of best fit we need to have some data first.  Write a function to generate some testing data within the range of 0 to 9 for both $x$ and $y$ axis.  Try visualising them by plotting them in Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have generated some random data with no correlation, it is very hard to find a linear connection between the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Generate Some 'Random' Data\n",
    "\n",
    "We should generate some data that follows a particular pattern.  Define variables gradient, y_intercept and max_offset.  Now, for each datapoint $(x, y)$, generate a random numbers k1 and k2 such that -max_offset $< k1, k2 <$ max_offset and a random number x that is between 0 and 9.  Your datapoint will be: $(x+k1, f(x)+k2)$ where f(x) is the function gradient * number + y_intercept.\n",
    "\n",
    "Display the data to verify that your solution looks like a linear line.  Make sure you do not have a large max_offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see that your data now looks like it is following some trend, but since we added some offset to each coordinates it doesnt look exactly like the line f(x).  Surely since we added the offset the line of best fit is not f(x) too.\n",
    "\n",
    "Now, let's try the hard way to find the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Linear Regression\n",
    "\n",
    "In Linear Regression, we use the Loss Function Mean Squared Error with the formula as the following: $\\sum (y-f(x))^2$.  Let us now try to implement it in PyTorch.\n",
    "\n",
    "For the task, we will be using SGD (Stochastic Gradient Descent) as the Optimizer and MSELoss for the loss.  Map all the data to numpy lists and turn them into tensors.  Define a weight that is defined to be a tensor with initialisation as 1 and bias as 0.  Tune the learning rate to be 1, which is considered quite high for most of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Training the Model\n",
    "\n",
    "Now we have the 'line of best fit' being w*x + b which looks nowhere like the line of best fit.  We have to fine tune the variables w and b so the 'line of best fit' actually is the line of best fit.  Train the model for 1000 epochs and see what happens, make sure that you output the loss every 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Hyperparameter Tuning\n",
    "\n",
    "Notice that the loss is increasing quite quickly.  This is because the learning rate is too large - remember the 'Hiking on a Misty Mountain Analogy'?  Having a large learning rate is like having large legs and you might not be able to go to the trough of the mountain.\n",
    "\n",
    "Try to tune the hyperparameter and find one that converges the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Display the line\n",
    "\n",
    "Plot on Matplotlib the Data that is generated and the result from Linear Regression, and display the line of best fit on the graph as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You finished the task!\n",
    "\n",
    "Feel free to mess about with the gradient and $y$ intercept to try out the program!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fffe788da19d770c4f2c54a6a5b17376000fd84ea27267c4b97ad15dbd02646"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the algorithm Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with all examples at root node\n",
    "2. calculate information gain for splitting on all possible features and pick one with highest value\n",
    "3. split the data based on the feature with highest information gain\n",
    "4. repeat the process until stopping criteria is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is a measure of randomness or unpredictability in the data. It is used to measure the impurity of the data. The entropy of the data is calculated as follows:\n",
    "\n",
    "$$Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2 p_i$$\n",
    "\n",
    "where $S$ is the dataset, $c$ is the number of classes, and $p_i$ is the probability of class $i$ in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information gain is the measure of decrease in entropy after the dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain. The information gain is calculated as follows:\n",
    "\n",
    "$$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$\n",
    "\n",
    "where $S$ is the dataset, $A$ is the attribute, $Values(A)$ is the set of all possible values of attribute $A$, and $S_v$ is the subset of $S$ for which attribute $A$ has value $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \"\"\"\n",
    "    A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Node class.\n",
    "\n",
    "        Args:\n",
    "            feature: The feature used for splitting at this node. Defaults to None.\n",
    "            threshold: The threshold used for splitting at this node. Defaults to None.\n",
    "            left: The left child node. Defaults to None.\n",
    "            right: The right child node. Defaults to None.\n",
    "            gain: The gain of the split. Defaults to None.\n",
    "            value: If this node is a leaf node, this attribute represents the predicted value\n",
    "                for the target variable. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    A decision tree classifier for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor for DecisionTree class.\n",
    "\n",
    "        Parameters:\n",
    "            min_samples (int): Minimum number of samples required to split an internal node.\n",
    "            max_depth (int): Maximum depth of the decision tree.\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def split_data(self, dataset, feature, threshold):\n",
    "        \"\"\"\n",
    "        Splits the given dataset into two datasets based on the given feature and threshold.\n",
    "\n",
    "        Parameters:\n",
    "            dataset (ndarray): Input dataset.\n",
    "            feature (int): Index of the feature to be split on.\n",
    "            threshold (float): Threshold value to split the feature on.\n",
    "\n",
    "        Returns:\n",
    "            left_dataset (ndarray): Subset of the dataset with values less than or equal to the threshold.\n",
    "            right_dataset (ndarray): Subset of the dataset with values greater than the threshold.\n",
    "        \"\"\"\n",
    "        # Create empty arrays to store the left and right datasets\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # Loop over each row in the dataset and split based on the given feature and threshold\n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "\n",
    "        # Convert the left and right datasets to numpy arrays and return\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        return left_dataset, right_dataset\n",
    "\n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of the given label values.\n",
    "\n",
    "        Parameters:\n",
    "            y (ndarray): Input label values.\n",
    "\n",
    "        Returns:\n",
    "            entropy (float): Entropy of the given label values.\n",
    "        \"\"\"\n",
    "        entropy = 0\n",
    "\n",
    "        # Find the unique label values in y and loop over each value\n",
    "        labels = np.unique(y)\n",
    "        for label in labels:\n",
    "            # Find the examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of the current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # Calculate the entropy using the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        # Return the final entropy value\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Computes the information gain from splitting the parent dataset into two datasets.\n",
    "\n",
    "        Parameters:\n",
    "            parent (ndarray): Input parent dataset.\n",
    "            left (ndarray): Subset of the parent dataset after split on a feature.\n",
    "            right (ndarray): Subset of the parent dataset after split on a feature.\n",
    "\n",
    "        Returns:\n",
    "            information_gain (float): Information gain of the split.\n",
    "        \"\"\"\n",
    "        # set initial information gain to 0\n",
    "        information_gain = 0\n",
    "        # compute entropy for parent\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        # calculate weight for left and right nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right= len(right) / len(parent)\n",
    "        # compute entropy for left and right nodes\n",
    "        entropy_left, entropy_right = self.entropy(left), self.entropy(right)\n",
    "        # calculate weighted entropy \n",
    "        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "        # calculate information gain \n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "\n",
    "    \n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to split.\n",
    "        num_samples (int): The number of samples in the dataset.\n",
    "        num_features (int): The number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with the best split feature index, threshold, gain, \n",
    "              left and right datasets.\n",
    "        \"\"\"\n",
    "        # dictionary to store the best split values\n",
    "        best_split = {'gain':- 1, 'feature': None, 'threshold': None}\n",
    "        # loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            #get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            #get unique values of that feature\n",
    "            thresholds = np.unique(feature_values)\n",
    "            # loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # get left and right datasets\n",
    "                left_dataset, right_dataset = self.split_data(dataset, feature_index, threshold)\n",
    "                # check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    # compute information gain based on the y values\n",
    "                    information_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "\n",
    "    \n",
    "    def calculate_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the most occurring value in the given list of y values.\n",
    "\n",
    "        Args:\n",
    "            y (list): The list of y values.\n",
    "\n",
    "        Returns:\n",
    "            The most occurring value in the list.\n",
    "        \"\"\"\n",
    "        y = list(y)\n",
    "        #get the highest present class in the array\n",
    "        most_occuring_value = max(y, key=y.count)\n",
    "        return most_occuring_value\n",
    "    \n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds a decision tree from the given dataset.\n",
    "\n",
    "        Args:\n",
    "        dataset (ndarray): The dataset to build the tree from.\n",
    "        current_depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        Node: The root node of the built decision tree.\n",
    "        \"\"\"\n",
    "        # split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # keeps spliting until stopping conditions are met\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best_split = self.best_split(dataset, n_samples, n_features)\n",
    "            # Check if gain isn't zero\n",
    "            if best_split[\"gain\"]:\n",
    "                # continue splitting the left and the right child. Increment current depth\n",
    "                left_node = self.build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "                right_node = self.build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"],\n",
    "                            left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "        # compute leaf node value\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        # return leaf node value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Builds and fits the decision tree to the given X and y values.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix.\n",
    "        y (ndarray): The target values.\n",
    "        \"\"\"\n",
    "        dataset = np.concatenate((X, y), axis=1)  \n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each instance in the feature matrix X.\n",
    "\n",
    "        Args:\n",
    "        X (ndarray): The feature matrix to make predictions for.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self.make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverses the decision tree to predict the target value for the given feature vector.\n",
    "\n",
    "        Args:\n",
    "        x (ndarray): The feature vector to predict the target value for.\n",
    "        node (Node): The current node being evaluated.\n",
    "\n",
    "        Returns:\n",
    "        The predicted target value for the given feature vector.\n",
    "        \"\"\"\n",
    "        # if the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None: \n",
    "            return node.value\n",
    "        else:\n",
    "            #if it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self.make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, random_state=41, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "        X (numpy.ndarray): Features array of shape (n_samples, n_features).\n",
    "        y (numpy.ndarray): Target array of shape (n_samples,).\n",
    "        random_state (int): Seed for the random number generator. Default is 42.\n",
    "        test_size (float): Proportion of samples to include in the test set. Default is 0.2.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray]: A tuple containing X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    # Get number of samples\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Set the seed for the random number generator\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle the indices\n",
    "    shuffled_indices = np.random.permutation(np.arange(n_samples))\n",
    "\n",
    "    # Determine the size of the test set\n",
    "    test_size = int(n_samples * test_size)\n",
    "\n",
    "    # Split the indices into test and train\n",
    "    test_indices = shuffled_indices[:test_size]\n",
    "    train_indices = shuffled_indices[test_size:]\n",
    "\n",
    "    # Split the features and target arrays into test and train\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "        float: The accuracy of the model\n",
    "    \"\"\"\n",
    "    y_true = y_true.flatten()\n",
    "    total_samples = len(y_true)\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    return (correct_predictions / total_samples) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate the balanced accuracy for a multi-class classification problem.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        y_true (numpy array): A numpy array of true labels for each data point.\n",
    "        y_pred (numpy array): A numpy array of predicted labels for each data point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        balanced_acc : The balanced accuracyof the model\n",
    "        \n",
    "    \"\"\"\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = y_true.flatten()\n",
    "    # Get the number of classes\n",
    "    n_classes = len(np.unique(y_true))\n",
    "\n",
    "    # Initialize an array to store the sensitivity and specificity for each class\n",
    "    sen = []\n",
    "    spec = []\n",
    "    # Loop over each class\n",
    "    for i in range(n_classes):\n",
    "        # Create a mask for the true and predicted values for class i\n",
    "        mask_true = y_true == i\n",
    "        mask_pred = y_pred == i\n",
    "\n",
    "        # Calculate the true positive, true negative, false positive, and false negative values\n",
    "        TP = np.sum(mask_true & mask_pred)\n",
    "        TN = np.sum((mask_true != True) & (mask_pred != True))\n",
    "        FP = np.sum((mask_true != True) & mask_pred)\n",
    "        FN = np.sum(mask_true & (mask_pred != True))\n",
    "\n",
    "        # Calculate the sensitivity (true positive rate) and specificity (true negative rate)\n",
    "        sensitivity = TP / (TP + FN)\n",
    "        specificity = TN / (TN + FP)\n",
    "\n",
    "        # Store the sensitivity and specificity for class i\n",
    "        sen.append(sensitivity)\n",
    "        spec.append(specificity)\n",
    "    # Calculate the balanced accuracy as the average of the sensitivity and specificity for each class\n",
    "    average_sen =  np.mean(sen)\n",
    "    average_spec =  np.mean(spec)\n",
    "    balanced_acc = (average_sen + average_spec) / n_classes\n",
    "\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=41, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model instance\n",
    "model = DecisionTree(2, 2)\n",
    "\n",
    "# Fit the decision tree model to the training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data.\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluating metrics\n",
    "print(f\"Model's Accuracy: {accuracy(y_test, predictions)}\")\n",
    "print(f\"Model's Balanced Accuracy: {balanced_accuracy(y_test, predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plthttps://docs.google.com/presentation/d/1-oYRj5svcJTsVZ7F_chzDcmTSKh-clNHdUF9496zJzA/edit#slide=id.g32d379c2a71_0_200\n",
    "import seaborn as sns\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the algorithm Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start with all examples at root node\n",
    "2. calculate information gain for splitting on all possible features and pick one with highest value\n",
    "3. split the data based on the feature with highest information gain\n",
    "4. repeat the process until stopping criteria is met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is a measure of randomness or unpredictability in the data. It is used to measure the impurity of the data. The entropy of the data is calculated as follows:\n",
    "\n",
    "$$Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2 p_i$$\n",
    "\n",
    "where:\n",
    "- $S$ is the dataset\n",
    "- $c$ is the number of classes\n",
    "- $p_i$ is the probability of class $i$ in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. if the data is pure, then the entropy is 0. If the data is equally distributed among all classes, then the entropy is 1.\n",
    "\n",
    "The more uncertain the outcome, the higher the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CoinFlip Example](entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information gain is the measure of decrease in entropy after the dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain. The information gain is calculated as follows:\n",
    "\n",
    "$$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$\n",
    "\n",
    "where \n",
    "- $S$ is the dataset\n",
    "- $A$ is the attribute\n",
    "- $Values(A)$ is the set of all possible values of attribute $A$\n",
    "- $S_v$ is the subset of $S$ for which attribute $A$ has value $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"breast-cancer.csv\")\n",
    "# run the code below if on google colab\n",
    "# df = pd.read_csv(\"/content/breast-cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation with target\n",
    "#encode the label into 1/0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features aren't correlated to the dataset - maybe we can drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features\n",
    "\n",
    "# Get the absolute value of the correlation\n",
    "\n",
    "\n",
    "# Select highly correlated features (thresold = 0.99)\n",
    "\n",
    "# Collect the names of the features\n",
    "\n",
    "# Drop the target variable from the results\n",
    "\n",
    "# Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Training data + Training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features from the dataframe\n",
    "\n",
    "# Get the target from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the data in the array X\n",
    "def scale(X):\n",
    "    # calculate mean and std of each feature\n",
    "\n",
    "    # standardize the data\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node class\n",
    "\n",
    "class Node():\n",
    "    # Class reprsenting a node in the decision tree. Assume binary splits\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        feature: int\n",
    "            The index of the feature used to make the split\n",
    "        threshold: float\n",
    "            The threshold value used to split the data\n",
    "        left: Node\n",
    "            The left child node\n",
    "        right: Node\n",
    "            The right child node\n",
    "        value: float\n",
    "            If leaf node, value represents predicted value for target variable\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Return string representation of node\n",
    "        \"\"\"\n",
    "        return f\"Feature: {self.feature}, Threshold: {self.threshold}, Value: {self.value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        min_samples: int\n",
    "            The minimum number of samples required to split an internal node\n",
    "        max_depth: int\n",
    "            The maximum depth of the tree\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples \n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def split_data(self, dataset, feature, threshold):\n",
    "        \"\"\"\n",
    "        Split the data based on the feature and threshold\n",
    "\n",
    "        dataset: np.array\n",
    "            The dataset to split\n",
    "        feature: int\n",
    "            The index of the feature to split on\n",
    "        threshold: float\n",
    "            The threshold value to split the data\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of the target variable\n",
    "\n",
    "        y: np.array\n",
    "            The target variable\n",
    "        \"\"\"\n",
    "        # Get the unique values and their counts\n",
    "\n",
    "        # Calculate the probabilities\n",
    "\n",
    "        # Calculate the entropy\n",
    "\n",
    "\n",
    "    def information_gain(self, parent, left_child, right_child):\n",
    "        \"\"\"\n",
    "        Calculate the information gain\n",
    "\n",
    "        parent: np.array\n",
    "            The parent node\n",
    "        left_child: np.array\n",
    "            The left child node\n",
    "        right_child: np.array\n",
    "            The right child node\n",
    "        \"\"\"\n",
    "        # Calculate the entropy of the parent\n",
    "\n",
    "        # Calculate the entropy of the children\n",
    "\n",
    "        # Calculate the information gain\n",
    "\n",
    "\n",
    "    def best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given dataset\n",
    "\n",
    "        dataset: np.array\n",
    "            The dataset to split\n",
    "        num_samples: int\n",
    "            The number of samples in the dataset\n",
    "        num_features: int\n",
    "            The number of features in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate over all features\n",
    "            # Get the unique values for the feature\n",
    "            # Iterate over all unique values\n",
    "                # Split the data\n",
    "\n",
    "                # If either split is empty, skip this split\n",
    "\n",
    "                # Calculate the information gain\n",
    "\n",
    "                # If the gain is better than the best gain, update the best split\n",
    "\n",
    "\n",
    "    def cal_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the most common value in the target variable\n",
    "\n",
    "        y: np.array\n",
    "            The target variable\n",
    "        \"\"\"\n",
    "        # Get the unique values and their counts\n",
    "\n",
    "        # Return the most common value\n",
    "\n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree\n",
    "\n",
    "        dataset: np.array\n",
    "            The dataset to split\n",
    "        current_depth: int\n",
    "            The current depth of the tree\n",
    "        \"\"\"\n",
    "\n",
    "        # If the stopping criteria is met, return a leaf node\n",
    "\n",
    "        # Find the best split\n",
    "\n",
    "        # If no split is found, return a leaf node\n",
    "\n",
    "        # Split the data\n",
    "\n",
    "        # Recursively build the tree\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree\n",
    "\n",
    "        X: np.array\n",
    "            The features\n",
    "        y: np.array\n",
    "            The target variable\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each instance in feature matrix X\n",
    "\n",
    "        X: np.array\n",
    "            The features\n",
    "        \"\"\"\n",
    "\n",
    "    def _predict(self, x, node):\n",
    "        \"\"\"\n",
    "        Recursively traverses decision tree to predict the class label\n",
    "\n",
    "        x: np.array\n",
    "            The features\n",
    "        node: Node\n",
    "            The current node\n",
    "        \"\"\"\n",
    "        # if leaf node, extract vlue\n",
    "\n",
    "        # if not leaf node, get feature it splits and threshold\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Return string representation of tree\n",
    "        \"\"\"\n",
    "        return self._print_tree(self.root)\n",
    "    \n",
    "    def _print_tree(self, node, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively print the tree\n",
    "\n",
    "        node: Node\n",
    "            The current node\n",
    "        depth: int\n",
    "            The current depth\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return \"\"\n",
    "        \n",
    "        if node.value is not None:\n",
    "            return f\"{node.value}\"\n",
    "        \n",
    "        output = \"\"\n",
    "        output += f\"{' ' * depth}Feature: {node.feature}, Threshold: {node.threshold}\\n\"\n",
    "        output += f\"{' ' * depth}Left: {self._print_tree(node.left, depth + 1)}\\n\"\n",
    "        output += f\"{' ' * depth}Right: {self._print_tree(node.right, depth + 1)}\\n\"\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, random_state=41, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets\n",
    "\n",
    "    X: np.array\n",
    "        The features\n",
    "    y: np.array\n",
    "        The target variable\n",
    "    random_state: int\n",
    "        The random seed for random number generator\n",
    "    test_size: float\n",
    "        The proportion of the data to include in the test split\n",
    "    \"\"\"\n",
    "    # need to randomise the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model\n",
    "    Note: we usually use the balanced accuracy score for imbalanced data (which is actually the case here but simplified for this tutorial)\n",
    "\n",
    "    y_true: np.array\n",
    "        The true target variable\n",
    "    y_pred: np.array\n",
    "        The predicted target variable\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# create model instance\n",
    "model = DecisionTree(min_samples=2, max_depth=2)\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# use trained model to predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "# calculate balanced accuracy \n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare inputs with outputs\n",
    "df = pd.DataFrame(data=X_test, columns=names)\n",
    "df['actual'] = y_test\n",
    "df['predicted'] = y_pred\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Random Forest?\n",
    "\n",
    "Decision trees are prone to overfitting. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "It is an example of an ensemble method, which is a model composed of multiple weaker models. The predictions of the individual models are combined to get the final prediction. More on this on week 8.\n",
    "\n",
    "## How Random Forest Works\n",
    "\n",
    "1. Randomly select *k* features from total *m* features where k << m\n",
    "2. Among the *k* features, create a decision tree using the best split point\n",
    "3. Build forest by repeating step 1 and 2 *n* times to create *n* decision trees\n",
    "4. The final prediction is calculated by averaging the predictions from all decision trees\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, num_trees=100, min_samples=2, max_depth=2, random_state=42):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        num_trees: int\n",
    "            The number of trees in the forest\n",
    "        min_samples: int\n",
    "            The minimum number of samples required to split an internal node\n",
    "        max_depth: int\n",
    "            The maximum depth of the tree\n",
    "        random_state: int\n",
    "            The random seed for random number generator\n",
    "        \"\"\"\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the random forest\n",
    "\n",
    "        X: np.array\n",
    "            The features\n",
    "        y: np.array\n",
    "            The target variable\n",
    "        \"\"\"\n",
    "        # for each tree in the forest\n",
    "            # create a decision tree\n",
    "            # create a bootstrap sample\n",
    "            # fit the decision tree\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        \"\"\"\n",
    "        Create a bootstrap sample\n",
    "\n",
    "        X: np.array\n",
    "            The features\n",
    "        y: np.array\n",
    "            The target variable\n",
    "        \"\"\"\n",
    "\n",
    "    def most_common_label(self, labels):\n",
    "        \"\"\"\n",
    "        Get the most common label in array of labels\n",
    "\n",
    "        labels: np.array\n",
    "            The labels\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each instance in feature matrix X\n",
    "\n",
    "        X: np.array\n",
    "            The features\n",
    "        \"\"\"\n",
    "        # get the most common prediction for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForest(num_trees=10, min_samples=2, max_depth=2)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "acc = accuracy(y_test, y_pred)\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Balanced Accuracy: {balanced_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show inputs and outputs as dataframe\n",
    "df = pd.DataFrame(data=X_test, columns=names)\n",
    "df['actual'] = y_test\n",
    "df['predicted'] = y_pred\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

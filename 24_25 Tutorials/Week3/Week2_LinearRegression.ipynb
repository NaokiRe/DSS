{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbHIgAd-RTO"
      },
      "source": [
        "# Linear Regression - PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MphlXQ0-RTR"
      },
      "source": [
        "### Importing relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "daHDxlsq-RTS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEwKweeO-RTT"
      },
      "source": [
        "### Generating Synthetic Data\n",
        "\n",
        "We first need to generate data to train our linear regression model on.\n",
        "\n",
        "In order to do so, `torch.arange` is called, which creates a tensor X containing a bunch of values between -5 and 5 with incremements of 0.1. This tensor is then shaped into a colomn vector, so you can think of X as a 1D vector containing all of the x co-ordinate data on the graph.\n",
        "\n",
        "Next, a tensor called func is created by applying a linear function of y = -5x. This applies the function y = -5x all of the elements of X, which generates the corresponding y co-ordinate for each x co-ordinate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qiLvlBKc-RTU"
      },
      "outputs": [],
      "source": [
        "# x co-ordinates of data points\n",
        "\n",
        "# y co-ordinates of data points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwU5fsL6-RTU"
      },
      "source": [
        "#### Noise\n",
        "\n",
        "We then add noise to the data points in func, to simulate a noisy data set.\n",
        "\n",
        "`torch.randn(X.size())` generates random numbers from a standard normal distribution of the shape of X, and this is then scaled by 0.4 to be added to the data points.\n",
        "\n",
        "These noise-adjusted values are stored in tensor Y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EkI90ezb-RTU"
      },
      "outputs": [],
      "source": [
        "#adding gaussian noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SnaUgpB-RTV"
      },
      "source": [
        "### Visualising Generated Data\n",
        "\n",
        "Now, we can use matplotlib to view our generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQIUg-zl-RTV",
        "outputId": "d5fb2d81-6997-47c6-ba40-721d5821505f"
      },
      "outputs": [],
      "source": [
        "plt.plot(X.numpy(), Y.numpy(), 'b+', label='Y')\n",
        "plt.plot(X.numpy(), func.numpy(), 'r', label='func')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid('True', color='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_iq8cGP-RTW"
      },
      "source": [
        "### Initialising Parameters\n",
        "\n",
        "In order to begin training, we first initialise some arbitrary values for a and b, where Y = a + bX. This is so that we have a starting point to begin training the data on. `a` and `b` are initialised as torch.tensor objects with requires_grad=True to track gradients during optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PYl9bdbm-RTW"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor(-20.0, requires_grad=True)\n",
        "b = torch.tensor(-10.0, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNM9QVWW-RTW"
      },
      "source": [
        "### Forward Pass Function\n",
        "\n",
        "Next we define a function called `forward(x)` which takes an input tensor `x` and computes the linear regression prediction using the parameters `a` and `b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SgEo2jor-RTX"
      },
      "outputs": [],
      "source": [
        "# forward pass function for prediction\n",
        "def forward(x):\n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Ite9S3-RTX"
      },
      "source": [
        "### Cost Criterion Function (MSE)\n",
        "\n",
        "In order to quantify the prediction accuracy, the mean squared error between the predicted values `y_pred` and `y` are calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JX80rd53-RTX"
      },
      "outputs": [],
      "source": [
        "def cost_function(y_pred, y):\n",
        "    return "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEUUxlxB-RTY"
      },
      "source": [
        "### Gradient Descent Optimisation\n",
        "\n",
        "Here, we define some training parameters.\n",
        "\n",
        "- learning_rate: size of the steps taken during each iteration of the process.\n",
        "- costs: stores a list to calculate the MSE cost at each iteration of the training loop (which we'll see below).\n",
        "- n_iter: this is the number of iterations (AKA epochs). Each epoch will involve a forward pass to compute predictions, a backward pass to calculate gradients and update parameters, and also will record the cost.\n",
        "\n",
        "*Please note that there is a trade off between speed of convergence and stability - a lower learning rate has slower convergence but is more stable, while a higher learning rate may have faster convergence, but with the added risk of overshooting the minimum*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "28fsTXcC-RTY"
      },
      "outputs": [],
      "source": [
        "# size of steps taken in each epoch\n",
        "learning_rate = 0.1\n",
        "#list to store loss values at each epoch\n",
        "costs = []\n",
        "# number of epochs\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gradient descent function\n",
        "\n",
        "def gradient_descent(x, y, learning_rate=0.1):\n",
        "\n",
        "    # forward pass\n",
        "    \n",
        "    # calculate cost\n",
        "    \n",
        "    # backward pass\n",
        "    \n",
        "    # updating the parameters after each epoch\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jesf0T4M-RTY"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "This is where the values of a and b are iterativly adjusted so as to reduce the average mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABuSAs3R-RTY"
      },
      "outputs": [],
      "source": [
        "for i in range (num_epochs):\n",
        "\n",
        "    \n",
        "\n",
        "    print('{}, \\t{}, \\t{}, \\t{}'.format(i, cost.item(), b.item(), a.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAGSEB3j-RTZ"
      },
      "outputs": [],
      "source": [
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "costs_np = [cost.detach().numpy() for cost in costs]\n",
        "plt.plot(epochs, costs_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extension - Linear Regression - From Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a more realistic example, where we have a dataset of multiple features and we want to predict the target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load in necessary csv files\n",
        "\n",
        "- If writing local code, no need to do this step\n",
        "\n",
        "If on Colab, download advertising.csv (in the content directory) and upload it to the current directory by:\n",
        "Clicking on Files icon (on the left side bar of the screen) -> Upload to session storage -> Select the file from your local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# variables to store mean and std for each feature\n",
        "mu = []\n",
        "std = []\n",
        "\n",
        "# Function to Normalise the data\n",
        "def normalise(data):    \n",
        "\tfor i in range(0,data.shape[1]-1):\n",
        "\t\tdata[:,i] = ((data[:,i] - np.mean(data[:,i]))/np.std(data[:, i]))\n",
        "\t\tmu.append(np.mean(data[:,i]))\n",
        "\t\tstd.append(np.std(data[:, i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(filename=\"/content/advertising.csv\"):\n",
        "    df = pd.read_csv(filename, sep=\",\", index_col=False)\n",
        "    # show the first 5 rows of the data\n",
        "    print(df.head())\n",
        "    data = np.array(df, dtype=float)\n",
        "    plot_TV_sales_data(data[:,:3], data[:, -1])\n",
        "    normalise(data)\n",
        "    return data[:,:3], data[:, -1]\n",
        "\n",
        "def plot_TV_sales_data(x, y):\n",
        "\tplt.xlabel('TV')\n",
        "\tplt.ylabel('sales')\n",
        "\tplt.plot(x[:,0], y, 'bo')\n",
        "\tplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data and show some values as pd dataframe\n",
        "x,y = load_data()\n",
        "# run this instead if local machine\n",
        "# x,y = load_data(\"./content/advertising.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we have to add a column of ones to the input data - because we have a bias term\n",
        "x = np.hstack([np.ones((x.shape[0], 1)), x])\n",
        "\n",
        "# initialize the weights\n",
        "a_b = np.zeros((x.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# re-define the forward pass function using matrix multiplication\n",
        "def forward(x, a_b):\n",
        "    return \n",
        "\n",
        "# re-define the cost function using matrix operations\n",
        "def cost_function(x, y, a_b):\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# re define gradient Descent\n",
        "\n",
        "def gradient_descent(x, y, a_b, learning_rate=0.1):\n",
        "    # get m - the number of data points\n",
        "    \n",
        "\n",
        "    # calculate the forward pass\n",
        "    \n",
        "\n",
        "    # calculate the cost\n",
        "    \n",
        "\n",
        "    # calculate the derivative of the cost function\n",
        "    \n",
        "\n",
        "    # update the parameters\n",
        "    \n",
        "\n",
        "\n",
        "    return a_b, cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The Training Loop\n",
        "\n",
        "cost_all = []\n",
        "learning_rate = 0.1\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    # calculate cost\n",
        "    \n",
        "\n",
        "    # append cost to list\n",
        "\n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print('{}, \\t{}'.format(i, cost[0][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for testing and plotting cost\n",
        "\n",
        "def plot_cost(cost_all, num_epochs):\n",
        "\tplt.xlabel('Epochs')\n",
        "\tplt.ylabel('Cost')\n",
        "\tplt.plot(num_epochs, cost_all, 'm', linewidth = \"5\")\n",
        "\tplt.show()\n",
        "\n",
        "n_epochs = []\n",
        "costplot = []\n",
        "count = 0\n",
        "for i in cost_all:\n",
        "\tcostplot.append(i[0][0])\n",
        "\tn_epochs.append(count)\n",
        "\tcount += 1\n",
        "costplot = np.array(costplot)\n",
        "n_epochs = np.array(n_epochs)\n",
        "plot_cost(costplot, n_epochs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

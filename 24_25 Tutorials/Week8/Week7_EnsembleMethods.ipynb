{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics as st\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ever so common Titanic dataset\n",
    "x_text = pd.read_csv('test.csv')\n",
    "y_test = pd.read_csv('gender_submission.csv')\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "x_train, y_train  = train.drop('Survived', axis=1), train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods workupon the principle that a group of weak learners come together to form a strong learner. The idea is to combine the predictions of multiple models to improve the overall performance. The ensemble methods are divided into three main categories:\n",
    "1. Bagging\n",
    "2. Stacking\n",
    "3. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Terminology\n",
    "- Base Learner / Base Model / Base Estimator: the individual models used in ensemble methods\n",
    "- Weak Learner: models that perform just above random chance (e.g. models with 50% accuracy)\n",
    "- Strong Learner: models that perform well above random chance (e.g. models with 80% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff\n",
    "\n",
    "Principle behind many different regularisation techniques. Defined as:\n",
    "- **Bias**: Average difference between the predicted values and the actual values. High bias models are oversimplified and do not capture the underlying patterns in the data. High bias = high error in training. Model predicts less accurately on seen data.\n",
    "- **Variance**: Variability of the model's prediction for a given data point. High variance models are too complex and capture noise in the training data. High variance = high error in testing. Model predicts less accurately on unseen data.\n",
    "- **Irreducible Error**: Error that cannot be reduced by model tuning. It is the error introduced by the noise in the data.\n",
    "\n",
    "We can define Error as:\n",
    "$Total Error = Bias^2 + Variance + Irreducible Error$\n",
    "\n",
    "### Reason for using many models\n",
    "Each model consists of numerous variables, each contributing to the total error. \n",
    "\n",
    "Even the same algorithm could yield models with different error rates. By combining multiple models, we can reduce the total error rate by averaging out the individual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Ensemble Methods\n",
    "\n",
    "Two types in general:\n",
    "- **Sequential Ensemble Methods**: Models are generated sequentially and each model tries to correct the errors of the previous model. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "- **Parallel Ensemble Methods**: Models are generated in parallel and then combined. Examples include Random Forest and Bagging.\n",
    "\n",
    "Parallel can be further divided into:\n",
    "- **Homogenous**: All models are of the same type. Example: Random Forest.\n",
    "- **Heterogenous**: Models are of different types. Example: Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques used in Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagging (Bootstrap Aggregating)\n",
    "Bagging is a parallel ensemble method that uses bootstrapping to create multiple datasets from the original dataset. Each dataset is then used to train a model. The final prediction is the average of all the predictions made by the individual models.\n",
    "\n",
    "bootstrap sampling is a technique in which we create multiple datasets by sampling with replacement from the original dataset. This means that some data points may be repeated in the new dataset, while others may not be included at all.\n",
    "\n",
    "Often used with decision trees, but can be used with any model.\n",
    "\n",
    "Possible to use multiple models but typically the same model is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BaggingExample](BaggingExample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class SimpleBaggingClassifier:\n",
    "    \"\"\"\n",
    "    A simple implementation of a Bagging Classifier\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_estimator : object\n",
    "        The base estimator to fit on random subsets of the dataset\n",
    "    n_estimators : int, default=10\n",
    "        The number of base estimators in the ensemble\n",
    "    random_state : int, default=None\n",
    "        Controls the random resampling of the original dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, n_estimators=10, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build a Bagging ensemble of estimators from the training data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # Convert X and y to numpy arrays if they're pandas dataframes\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Use random_state for reproducibility\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Create and train each estimator\n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap sampling - randomly select samples with replacement\n",
    "            # This creates a dataset of same size but with duplicates\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            \n",
    "            # Create a clone of the base estimator\n",
    "            estimator = type(self.base_estimator)()\n",
    "            \n",
    "            # Train the estimator on the bootstrap sample\n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "            \n",
    "            # Add the trained estimator to our collection\n",
    "            self.estimators.append(estimator)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X using majority voting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The predicted classes\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        # Get predictions from each estimator\n",
    "        predictions = np.array([estimator.predict(X) for estimator in self.estimators])\n",
    "        \n",
    "        # Transpose to get a samples x estimators matrix\n",
    "        predictions = predictions.T\n",
    "        \n",
    "        # Apply majority voting - take the most common prediction for each sample\n",
    "        final_predictions = np.array([st.mode(pred) for pred in predictions])\n",
    "        \n",
    "        return final_predictions\n",
    "\n",
    "# Example usage\n",
    "# Let's prepare the data first\n",
    "# For simplicity, we'll use only numerical features and handle missing values\n",
    "# Let's preprocess the data\n",
    "def preprocess_data(data):\n",
    "    # Select only numerical features for simplicity\n",
    "    numerical_features = ['Age', 'Fare', 'Pclass']\n",
    "    X = data[numerical_features].copy()\n",
    "    \n",
    "    # Fill missing values with mean\n",
    "    X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "    X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed = preprocess_data(x_train)\n",
    "X_test_processed = preprocess_data(x_text)\n",
    "\n",
    "# Create and train our bagging classifier\n",
    "# Using Decision Tree as base estimator\n",
    "base_tree = DecisionTreeClassifier(max_depth=3)\n",
    "bagging_clf = SimpleBaggingClassifier(base_estimator=base_tree, n_estimators=10, random_state=42)\n",
    "bagging_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = bagging_clf.predict(X_test_processed)\n",
    "\n",
    "# Compare with actual values\n",
    "accuracy = np.mean(predictions == y_test['Survived'])\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn's implementation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "sklearn_bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=3),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_bagging.fit(X_train_processed, y_train)\n",
    "sklearn_predictions = sklearn_bagging.predict(X_test_processed)\n",
    "sklearn_accuracy = np.mean(sklearn_predictions == y_test['Survived'])\n",
    "print(f\"Scikit-learn's Bagging Classifier Accuracy: {sklearn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stacking (Stacked Generalization)\n",
    "\n",
    "Stacking is a parallel ensemble method that combines multiple models using a meta-learner. The meta-learner takes the predictions of the individual models as input and makes the final prediction.\n",
    "\n",
    "The idea is to use the predictions of the base models as input features for the meta-learner. The meta-learner then learns to combine the predictions of the base models to make the final prediction.\n",
    "\n",
    "The base models are often diverse in nature, meaning that they are different types of models or models trained on different subsets of the data. This helps to reduce the correlation between the base models and improve the performance of the ensemble.\n",
    "\n",
    "The meta-learner can be any model, but it is often a simple linear model or a tree-based model. This helps to reduce the complexity of the ensemble and divert the main inference\n",
    "to the base models.\n",
    "\n",
    "\n",
    "![StackingExample](StackingExample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class SimpleStackingClassifier:\n",
    "    \"\"\"\n",
    "    A simple implementation of a Stacking Classifier\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_estimators : list\n",
    "        List of base estimator objects\n",
    "    meta_estimator : object\n",
    "        The meta estimator to combine the predictions of base estimators\n",
    "    n_folds : int, default=5\n",
    "        Number of folds for cross-validation\n",
    "    random_state : int, default=None\n",
    "        Controls the randomness in CV split\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimators, meta_estimator, n_folds=5, random_state=None):\n",
    "        self.base_estimators = base_estimators\n",
    "        self.meta_estimator = meta_estimator\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        self.trained_base_estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the stacking ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # Convert X and y to numpy arrays if they're pandas dataframes\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_estimators = len(self.base_estimators)\n",
    "        \n",
    "        # Create array to hold meta-features (predictions from base models)\n",
    "        meta_features = np.zeros((n_samples, n_estimators))\n",
    "        \n",
    "        # Split data for cross-validation\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # For each base estimator, generate out-of-fold predictions\n",
    "        for i, estimator in enumerate(self.base_estimators):\n",
    "            print(f\"Training base estimator {i+1}/{n_estimators}: {type(estimator).__name__}\")\n",
    "            \n",
    "            # We'll collect out-of-fold predictions here\n",
    "            temp_meta_features = np.zeros(n_samples)\n",
    "            \n",
    "            # Create a fresh instance for each fold\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                # Get training and validation sets for this fold\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Clone the estimator to start fresh\n",
    "                fold_estimator = type(estimator)()\n",
    "                \n",
    "                # Train on the training portion\n",
    "                fold_estimator.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Make predictions on validation portion\n",
    "                temp_meta_features[val_idx] = fold_estimator.predict(X_val_fold)\n",
    "            \n",
    "            # Store the meta-features (out-of-fold predictions)\n",
    "            meta_features[:, i] = temp_meta_features\n",
    "            \n",
    "            # Train a final model on all data for future predictions\n",
    "            final_estimator = type(estimator)()\n",
    "            final_estimator.fit(X, y)\n",
    "            self.trained_base_estimators.append(final_estimator)\n",
    "        \n",
    "        # Train the meta-estimator using the meta-features\n",
    "        print(f\"Training meta estimator: {type(self.meta_estimator).__name__}\")\n",
    "        self.meta_estimator.fit(meta_features, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X using the stacked ensemble\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The predicted classes\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_estimators = len(self.base_estimators)\n",
    "        \n",
    "        # Create array to hold meta-features for prediction\n",
    "        meta_features = np.zeros((n_samples, n_estimators))\n",
    "        \n",
    "        # Generate meta-features using trained base estimators\n",
    "        for i, estimator in enumerate(self.trained_base_estimators):\n",
    "            meta_features[:, i] = estimator.predict(X)\n",
    "        \n",
    "        # Make final predictions using the meta-estimator\n",
    "        return self.meta_estimator.predict(meta_features)\n",
    "\n",
    "# Example usage\n",
    "# Let's use the same preprocessing as in the bagging example\n",
    "def preprocess_data(data):\n",
    "    # Select only numerical features for simplicity\n",
    "    numerical_features = ['Age', 'Fare', 'Pclass']\n",
    "    X = data[numerical_features].copy()\n",
    "    \n",
    "    # Fill missing values with mean\n",
    "    X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "    X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed = preprocess_data(x_train)\n",
    "X_test_processed = preprocess_data(x_text)\n",
    "\n",
    "# Create base estimators (weak learners) of different types\n",
    "base_estimators = [\n",
    "    DecisionTreeClassifier(max_depth=3),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    LogisticRegression(max_iter=1000)\n",
    "]\n",
    "\n",
    "# Create meta estimator\n",
    "meta_estimator = LogisticRegression()\n",
    "\n",
    "# Create and train our stacking classifier\n",
    "stacking_clf = SimpleStackingClassifier(\n",
    "    base_estimators=base_estimators,\n",
    "    meta_estimator=meta_estimator,\n",
    "    n_folds=5,\n",
    "    random_state=42\n",
    ")\n",
    "stacking_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = stacking_clf.predict(X_test_processed)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test['Survived'])\n",
    "print(f\"\\nStacking Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nIndividual Model Performances:\")\n",
    "for i, estimator in enumerate(base_estimators):\n",
    "    # Train the individual model\n",
    "    model = type(estimator)()\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    # Make predictions\n",
    "    ind_predictions = model.predict(X_test_processed)\n",
    "    # Calculate accuracy\n",
    "    ind_accuracy = np.mean(ind_predictions == y_test['Survived'])\n",
    "    print(f\"{type(model).__name__} Accuracy: {ind_accuracy:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn's StackingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "sklearn_stacking = StackingClassifier(\n",
    "    estimators=[(f\"est{i}\", type(est)()) for i, est in enumerate(base_estimators)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "sklearn_stacking.fit(X_train_processed, y_train)\n",
    "sklearn_predictions = sklearn_stacking.predict(X_test_processed)\n",
    "sklearn_accuracy = np.mean(sklearn_predictions == y_test['Survived'])\n",
    "print(f\"\\nScikit-learn's StackingClassifier Accuracy: {sklearn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "Boosting is a sequential ensemble method that combines multiple weak learners to create a strong learner. The idea is to train the models sequentially, with each model trying to correct the errors of the previous model.\n",
    "\n",
    "The key idea behind boosting is to assign weights to the data points based on their importance. The weights are updated at each iteration to give more importance to the data points that were misclassified by the previous models.\n",
    "\n",
    "The final prediction is made by combining the predictions of all the models, with more weight given to the models that perform better on the training data.\n",
    "\n",
    "Boosting is often used with decision trees, but it can be used with any model.\n",
    "\n",
    "There are many different boosting algorithms, including AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "![BoostingExample](BoostingExample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost is a boosting algorithm that assigns weights to the data points based on their importance. The weights are updated at each iteration to give more importance to the data points that were misclassified by the previous models.\n",
    "\n",
    "The final prediction is made by combining the predictions of all the models, with more weight given to the models that perform better on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class SimpleAdaBoostClassifier:\n",
    "    \"\"\"\n",
    "    A simple implementation of AdaBoost classifier\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_estimator : object\n",
    "        The base estimator to use for boosting (weak learner)\n",
    "    n_estimators : int, default=50\n",
    "        The maximum number of estimators to use\n",
    "    learning_rate : float, default=1.0\n",
    "        Weight applied to each classifier at each boosting iteration\n",
    "    random_state : int, default=None\n",
    "        Controls the random seed for base estimator's randomness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, n_estimators=50, learning_rate=1.0, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Will store the collection of estimators\n",
    "        self.estimators = []\n",
    "        # Will store the weight of each estimator in the final prediction\n",
    "        self.estimator_weights = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the AdaBoost classifier from the training data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values (class labels must be -1 and 1)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # Convert X and y to numpy arrays if they're pandas dataframes\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Make sure y contains -1 and 1 (required for AdaBoost)\n",
    "        # For binary classification with 0 and 1, convert 0 to -1\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) != 2:\n",
    "            raise ValueError(\"AdaBoost requires binary classification\")\n",
    "        \n",
    "        # Map labels to -1 and 1 if needed\n",
    "        y_transformed = y.copy()\n",
    "        if not np.all(np.isin(unique_classes, [-1, 1])):\n",
    "            # Assume binary classification with labels like 0 and 1\n",
    "            y_transformed = np.where(y_transformed == unique_classes[0], -1, 1)\n",
    "            print(f\"Mapped classes {unique_classes} to [-1, 1]\")\n",
    "        \n",
    "        # Initialize sample weights (uniform distribution)\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(self.n_estimators):\n",
    "            print(f\"Training estimator {i+1}/{self.n_estimators}\")\n",
    "            \n",
    "            # Clone the base estimator\n",
    "            estimator = type(self.base_estimator)()\n",
    "            \n",
    "            # Some estimators in scikit-learn support sample_weight directly\n",
    "            try:\n",
    "                estimator.fit(X, y_transformed, sample_weight=sample_weights)\n",
    "            except TypeError:\n",
    "                # If sample_weight is not supported, use weighted sampling\n",
    "                # Sample indices according to sample weights\n",
    "                indices = np.random.choice(\n",
    "                    n_samples, \n",
    "                    size=n_samples, \n",
    "                    replace=True, \n",
    "                    p=sample_weights\n",
    "                )\n",
    "                estimator.fit(X[indices], y_transformed[indices])\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = estimator.predict(X)\n",
    "            \n",
    "            # Ensure predictions are -1 and 1\n",
    "            if not np.all(np.isin(np.unique(predictions), [-1, 1])):\n",
    "                predictions = np.where(predictions == unique_classes[0], -1, 1)\n",
    "            \n",
    "            # Calculate the weighted error\n",
    "            incorrect = predictions != y_transformed\n",
    "            error = np.sum(sample_weights * incorrect) / np.sum(sample_weights)\n",
    "            \n",
    "            # If error is too large (≥0.5), this estimator is no better than random guessing\n",
    "            if error >= 0.5:\n",
    "                print(f\"  Error rate {error:.4f} ≥ 0.5, stopping early\")\n",
    "                break\n",
    "            \n",
    "            # Calculate estimator weight\n",
    "            estimator_weight = self.learning_rate * np.log((1 - error) / max(error, 1e-10))\n",
    "            \n",
    "            # Update sample weights\n",
    "            sample_weights = sample_weights * np.exp(estimator_weight * incorrect)\n",
    "            \n",
    "            # Normalize sample weights to sum to 1\n",
    "            sample_weights = sample_weights / np.sum(sample_weights)\n",
    "            \n",
    "            # Store the estimator and its weight\n",
    "            self.estimators.append(estimator)\n",
    "            self.estimator_weights.append(estimator_weight)\n",
    "            \n",
    "            print(f\"  Error rate: {error:.4f}, Estimator weight: {estimator_weight:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X using the weighted estimators\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The predicted classes\n",
    "        \"\"\"\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Get weighted sum of predictions from all estimators\n",
    "        weighted_predictions = np.zeros(n_samples)\n",
    "        \n",
    "        for estimator, weight in zip(self.estimators, self.estimator_weights):\n",
    "            predictions = estimator.predict(X)\n",
    "            \n",
    "            # Ensure predictions are -1 and 1\n",
    "            if not np.all(np.isin(np.unique(predictions), [-1, 1])):\n",
    "                # Convert to -1 and 1 if needed\n",
    "                unique_values = np.unique(predictions)\n",
    "                if len(unique_values) == 2:\n",
    "                    predictions = np.where(predictions == unique_values[0], -1, 1)\n",
    "            \n",
    "            weighted_predictions += weight * predictions\n",
    "        \n",
    "        # Convert back to original class labels (0 and 1) if needed\n",
    "        # Positive score -> class 1, Negative score -> class 0\n",
    "        return np.where(weighted_predictions >= 0, 1, 0)\n",
    "\n",
    "# Example usage\n",
    "# Preprocess training and test data (reusing the preprocessing function from earlier)\n",
    "def preprocess_data(data):\n",
    "    # Select only numerical features for simplicity\n",
    "    numerical_features = ['Age', 'Fare', 'Pclass']\n",
    "    X = data[numerical_features].copy()\n",
    "    \n",
    "    # Fill missing values with mean\n",
    "    X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "    X['Fare'].fillna(X['Fare'].mean(), inplace=True)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Preprocess training and test data\n",
    "X_train_processed = preprocess_data(x_train)\n",
    "X_test_processed = preprocess_data(x_text)\n",
    "\n",
    "# Create and train our AdaBoost classifier\n",
    "# Using Decision Tree with limited depth as base estimator (weak learner)\n",
    "base_tree = DecisionTreeClassifier(max_depth=1)  # Decision stump (typical weak learner for AdaBoost)\n",
    "adaboost_clf = SimpleAdaBoostClassifier(\n",
    "    base_estimator=base_tree,\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "adaboost_clf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = adaboost_clf.predict(X_test_processed)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test['Survived'])\n",
    "print(f\"\\nAdaBoost Classifier Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Compare with base estimator (single decision stump)\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "base_model.fit(X_train_processed, y_train)\n",
    "base_predictions = base_model.predict(X_test_processed)\n",
    "base_accuracy = np.mean(base_predictions == y_test['Survived'])\n",
    "print(f\"Base Estimator (Decision Stump) Accuracy: {base_accuracy:.4f}\")\n",
    "\n",
    "# Compare with scikit-learn's AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "sklearn_adaboost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "sklearn_adaboost.fit(X_train_processed, y_train)\n",
    "sklearn_predictions = sklearn_adaboost.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

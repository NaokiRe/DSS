{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods workupon the principle that a group of weak learners come together to form a strong learner. The idea is to combine the predictions of multiple models to improve the overall performance. The ensemble methods are divided into two categories:\n",
    "1. Bagging\n",
    "2. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Terminology\n",
    "- Base Learner / Base Model / Base Estimator: the individual models used in ensemble methods\n",
    "- Weak Learner: models that perform just above random chance (e.g. models with 50% accuracy)\n",
    "- Strong Learner: models that perform well above random chance (e.g. models with 80% accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Tradeoff\n",
    "\n",
    "Principle behind many different regularisation techniques. Defined as:\n",
    "- **Bias**: Average difference between the predicted values and the actual values. High bias models are oversimplified and do not capture the underlying patterns in the data. High bias = high error in training. Model predicts less accurately on seen data.\n",
    "- **Variance**: Variability of the model's prediction for a given data point. High variance models are too complex and capture noise in the training data. High variance = high error in testing. Model predicts less accurately on unseen data.\n",
    "- **Irreducible Error**: Error that cannot be reduced by model tuning. It is the error introduced by the noise in the data.\n",
    "\n",
    "We can define Error as:\n",
    "$Total Error = Bias^2 + Variance + Irreducible Error$\n",
    "\n",
    "### Reason for using many models\n",
    "Each model consists of numerous variables, each contributing to the total error. \n",
    "\n",
    "Even the same algorithm could yield models with different error rates. By combining multiple models, we can reduce the total error rate by averaging out the individual errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Ensemble Methods\n",
    "\n",
    "Two types in general:\n",
    "- **Sequential Ensemble Methods**: Models are generated sequentially and each model tries to correct the errors of the previous model. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "- **Parallel Ensemble Methods**: Models are generated in parallel and then combined. Examples include Random Forest and Bagging.\n",
    "\n",
    "Parallel can be further divided into:\n",
    "- **Homogenous**: All models are of the same type. Example: Random Forest.\n",
    "- **Heterogenous**: Models are of different types. Example: Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques used in Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
